version: v0.1
listeners:
  ingress_traffic:
    address: 0.0.0.0
    port: 10000
    message_format: openai
    timeout: 30s

# Centralized way to manage LLMs, manage keys, retry logic, failover and limits in a central way
llm_providers:
  - name: OpenAI
    provider_interface: openai
    access_key: $OPENAI_API_KEY
    model: gpt-4o
    default: true

# default system prompt used by all prompt targets
system_prompt: |
  You are a network assistant that helps operators with a better understanding of network traffic flow and perform actions on networking operations. No advice on manufacturers or purchasing decisions.

prompt_targets:
  - name: device_summary
    description: Retrieve network statistics for specific devices within a time range
    endpoint:
      name: app_server
      path: /agent/device_summary
      http_method: POST
    parameters:
      - name: device_id
        type: str
        description: A device identifier to retrieve statistics for.
        required: true # device_ids are required to get device statistics
      - name: days
        type: int
        description: The number of days for which to gather device statistics.
        default: 7
  - name: reboot_device
    description: Reboot a device
    endpoint:
      name: app_server
      path: /agent/device_reboot
      http_method: POST
    parameters:
      - name: device_id
        type: str
        description: the device identifier
        required: true
    system_prompt: You will get a status JSON object. Simply summarize it

# Arch creates a round-robin load balancing between different endpoints, managed via the cluster subsystem.
endpoints:
  app_server:
    # value could be ip address or a hostname with port
    # this could also be a list of endpoints for load balancing
    # for example endpoint: [ ip1:port, ip2:port ]
    endpoint: host.docker.internal:18083
    # max time to wait for a connection to be established
    connect_timeout: 0.005s


tracing:
  random_sampling: 100
  trace_arch_internal: true
